
{
 "cells": [
      
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    var SparkConf = require('eclairjs\/SparkConf');\n",
    "    var SparkContext = require('eclairjs\/SparkContext');\n",
    "    var sparkConf = new SparkConf().setAppName(\"JavaScript Estimator Transformer Param Example\");\n",
    "    var sc = new SparkContext(sparkConf);\n",
    "\n",
    "    var SQLContext = require('eclairjs\/sql\/SQLContext');\n",
    "    var RowFactory = require('eclairjs\/sql\/RowFactory');\n",
    "    var DataTypes = require('eclairjs\/sql\/types\/DataTypes');\n",
    "    var Vectors = require('eclairjs\/mllib\/linalg\/Vectors');\n",
    "    var VectorUDT = require('eclairjs\/mllib\/linalg\/VectorUDT');\n",
    "    var LabeledPoint = require('eclairjs\/mllib\/regression\/LabeledPoint');\n",
    "    var ParamMap = require('eclairjs\/ml\/param\/ParamMap');\n",
    "    var LogisticRegression = require('eclairjs\/ml\/classification\/LogisticRegression');\n",
    "\n",
    "    var result = {};\n",
    "    var sqlContext = new SQLContext(sc);\n",
    "    var fields = [\n",
    "        DataTypes.createStructField(\"label\", DataTypes.DoubleType, false),\n",
    "        DataTypes.createStructField(\"features\", new VectorUDT(), true)\n",
    "    ];\n",
    "\n",
    "    var schema = DataTypes.createStructType(fields);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Prepare training data.\n",
    " DataFrames, where it uses the bean metadata to infer the schema.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var training = sqlContext.createDataFrame(\n",
    "        [\n",
    "            RowFactory.create(1.0, Vectors.dense(0.0, 1.1, 0.1)),\n",
    "            RowFactory.create(0.0, Vectors.dense(2.0, 1.1, -1.0)),\n",
    "            RowFactory.create(0.0, Vectors.dense(2.0, 1.3, 1.0)),\n",
    "            RowFactory.create(1.0, Vectors.dense(0.0, 1.2, -0.5))\n",
    "        ], schema);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a LogisticRegression instance. This instance is an Estimator.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var lr = new LogisticRegression();\n",
    "    result.lr_explainParams = lr.explainParams();\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We may set parameters using setter methods.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    lr.setMaxIter(10).setRegParam(0.01);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Learn a LogisticRegression model. This uses the parameters stored in lr.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var model1 = lr.fit(training);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Since model1 is a Model (i.e., a Transformer produced by an Estimator),\n",
    " we can view the parameters it used during fit().\n",
    " This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
    " LogisticRegression instance.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    result.model1_extractParamMap = model1.parent().extractParamMap();\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We may alternatively specify parameters using a ParamMap.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var paramMap = new ParamMap()\n",
    "        .put(lr.maxIter().w(20))  \/\/ Specify 1 Param.\n",
    "        .put(lr.maxIter(), 30)  \/\/ This overwrites the original maxIter.\n",
    "        .put(lr.regParam().w(0.1), lr.threshold().w(0.55));  \/\/ Specify multiple Params.\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " One can also combine ParamMaps.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var paramMap2 = new ParamMap()\n",
    "        .put(lr.probabilityCol().w(\"myProbability\"));  \/\/ Change output column name\n",
    "    var paramMapCombined = paramMap.$plus$plus(paramMap2);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now learn a new model using the paramMapCombined parameters.\n",
    " paramMapCombined overrides all parameters set earlier via lr.set* methods.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var model2 = lr.fit(training, paramMapCombined);\n",
    "    result.model2_extractParamMap = model2.parent().extractParamMap();\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Prepare test documents.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "   var test = sqlContext.createDataFrame([\n",
    "        RowFactory.create(1.0, Vectors.dense(-1.0, 1.5, 1.3)),\n",
    "        RowFactory.create(0.0, Vectors.dense(3.0, 2.0, -0.1)),\n",
    "        RowFactory.create(1.0, Vectors.dense(0.0, 2.2, -1.5))\n",
    "    ], schema);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Make predictions on test documents using the Transformer.transform() method.\n",
    " LogisticRegression.transform will only use the 'features' column.\n",
    " Note that model2.transform() outputs a 'myProbability' column instead of the usual\n",
    " 'probability' column since we renamed the lr.probabilityCol parameter previously.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var results = model2.transform(test);\n",
    "\n",
    "    result.rows = results.select(\"features\", \"label\", \"myProbability\", \"prediction\").collect();\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Print out the parameters, documentation, and any default values.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    print(\"LogisticRegression parameters:\\n\" + result.lr_explainParams + \"\\n\");\n",
    "    print(\"Model 1 was fit using parameters: \" + result.model1_extractParamMap);\n",
    "    print(\"Model 2 was fit using parameters: \" + result.model2_extractParamMap);\n",
    "    result.rows.forEach(function(r){\n",
    "        print(\"(\" + r.get(0) + \", \" + r.get(1) + \") -> prob=\" + r.get(2)\n",
    "            + \", prediction=\" + r.get(3));\n",
    "    });\n",
    "\n",
    "    sc.stop();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.6.1 (Javascript)",
   "language": "javascript",
   "name": "eclair"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}