
{
 "cells": [
      
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var SparkConf = require('eclairjs\/SparkConf');\n",
    "    var SparkContext = require('eclairjs\/SparkContext');\n",
    "\n",
    "    var sparkConf = new SparkConf().setAppName(\"Example\");\n",
    "    var sc = new SparkContext(sparkConf);\n",
    "    var SQLContext = require('eclairjs\/sql\/SQLContext');\n",
    "    var RowFactory = require('eclairjs\/sql\/RowFactory');\n",
    "    var StructType = require(\"eclairjs\/sql\/types\/StructType\");\n",
    "    var StructField = require(\"eclairjs\/sql\/types\/StructField\");\n",
    "    var DataTypes = require(\"eclairjs\/sql\/types\/DataTypes\");\n",
    "    var Metadata = require(\"eclairjs\/sql\/types\/Metadata\");\n",
    "    var Tokenizer = require(\"eclairjs\/ml\/feature\/Tokenizer\");\n",
    "    var HashingTF = require(\"eclairjs\/ml\/feature\/HashingTF\");\n",
    "    var LogisticRegression = require(\"eclairjs\/ml\/classification\/LogisticRegression\");\n",
    "    var Pipeline = require(\"eclairjs\/ml\/Pipeline\");\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var sqlContext = new SQLContext(sc);\n",
    "\n",
    "    var schema = new StructType([\n",
    "        new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n",
    "        new StructField(\"text\", DataTypes.StringType, false, Metadata.empty()),\n",
    "        new StructField(\"label\", DataTypes.DoubleType, false, Metadata.empty())\n",
    "    ]);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Prepare training documents, which are labeled.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var training = sqlContext.createDataFrame([\n",
    "        RowFactory.create(0, \"a b c d e spark\", 1.0),\n",
    "        RowFactory.create(1, \"b d\", 0.0),\n",
    "        RowFactory.create(2, \"spark f g h\", 1.0),\n",
    "        RowFactory.create(3, \"hadoop mapreduce\", 0.0)\n",
    "    ], schema);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var tokenizer = new Tokenizer()\n",
    "        .setInputCol(\"text\")\n",
    "        .setOutputCol(\"words\");\n",
    "    var hashingTF = new HashingTF()\n",
    "        .setNumFeatures(1000)\n",
    "        .setInputCol(tokenizer.getOutputCol())\n",
    "        .setOutputCol(\"features\");\n",
    "    var lr = new LogisticRegression()\n",
    "        .setMaxIter(10)\n",
    "        .setRegParam(0.01);\n",
    "    var pipeline = new Pipeline()\n",
    "        .setStages([tokenizer, hashingTF, lr]);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Fit the pipeline to training documents.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var model = pipeline.fit(training);\n",
    "\n",
    "    var schema2 = new StructType([\n",
    "        new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n",
    "        new StructField(\"text\", DataTypes.StringType, false, Metadata.empty())\n",
    "    ]);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Prepare test documents, which are unlabeled.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var test = sqlContext.createDataFrame([\n",
    "        RowFactory.create(4, \"spark i j k\"),\n",
    "        RowFactory.create(5, \"l m n\"),\n",
    "        RowFactory.create(6, \"mapreduce spark\"),\n",
    "        RowFactory.create(7, \"apache hadoop\")\n",
    "    ], schema2);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Make predictions on test documents.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var predictions = model.transform(test);\n",
    "    var rows = predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collect();\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var result = rows;\n",
    "    result.forEach(function (r) {\n",
    "        print(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n",
    "            + \", prediction=\" + r.get(3));\n",
    "    });\n",
    "\n",
    "    sc.stop();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.6.1 (Javascript)",
   "language": "javascript",
   "name": "eclair"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}