
{
 "cells": [
      
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var LDA = require('eclairjs\/mllib\/clustering\/LDA');\n",
    "var Vectors = require('eclairjs\/mllib\/linalg\/Vectors');\n",
    "var Tuple2 = require('eclairjs\/Tuple2');\n",
    "var PairRDD = require('eclairjs\/PairRDD');\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    var SparkConf = require('eclairjs\/SparkConf');\n",
    "    var SparkContext = require('eclairjs\/SparkContext');\n",
    "    var sparkConf = new SparkConf().setAppName(\"LDA Example\");\n",
    "    var sc = new SparkContext(sparkConf);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Load and parse the data\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var path =  \"..\/..\/data\/mllib\/sample_lda_data.txt\";\n",
    "    var data = sc.textFile(path);\n",
    "    var parsedData = data.map(function (s, Vectors) {\n",
    "       var sarray = s.trim().split(\" \");\n",
    "        var values = [];\n",
    "        for (var i = 0; i < sarray.length; i++) {\n",
    "            values[i] = parseFloat(sarray[i]);\n",
    "        }\n",
    "        return Vectors.dense(values);\n",
    "    }, [Vectors]);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Index documents with unique IDs\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var data = parsedData.zipWithIndex().map(function (doc_id, Tuple2) {\n",
    "        return new Tuple2(doc_id._2(), doc_id._1()); \/\/ swap\n",
    "    }, [Tuple2]);\n",
    "    var corpus = PairRDD.fromRDD(data).cache();\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Cluster the documents into three topics using LDA\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var ldaModel = new LDA().setK(3).run(corpus);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Output topics. Each is a distribution over words (matching word count vectors)\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var ret = {};\n",
    "    ret.vocabSize = ldaModel.vocabSize();\n",
    "    ret.topics = [];\n",
    "\n",
    "    var topics = ldaModel.topicsMatrix();\n",
    "    for (var topic = 0; topic < 3; topic++) {\n",
    "        var str = \"\";\n",
    "        for (var word = 0; word < ldaModel.vocabSize(); word++) {\n",
    "            str += \" \" + topics.apply(word, topic);\n",
    "        }\n",
    "        ret.topics[topic] = str;\n",
    "    }\n",
    "var result = ret;\n",
    "    print(\"Learned topics (as distributions over vocab of \" + result.vocabSize + \" words):\");\n",
    "    for (var t in result.topics) {\n",
    "        print(\"Topic \" + t + \":\");\n",
    "        print(result.topics[t]);\n",
    "    }\n",
    "\n",
    "    sc.stop();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.6.1 (Javascript)",
   "language": "javascript",
   "name": "eclair"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}