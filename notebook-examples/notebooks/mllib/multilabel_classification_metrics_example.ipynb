
{
 "cells": [
      
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "var MultilabelMetrics = require('eclairjs\/mllib\/evaluation').MultilabelMetrics;\n",
    "var Tuple2 = require('eclairjs\/Tuple2');\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var SparkConf = require('eclairjs\/SparkConf');\n",
    "    var SparkContext = require('eclairjs\/SparkContext');\n",
    "    var sparkConf = new SparkConf().setAppName(\"Multilabel Classification Metrics Example\");\n",
    "    var sc = new SparkContext(sparkConf);\n",
    "\n",
    "    var data = [\n",
    "        new Tuple2([0.0, 1.0], [0.0, 2.0]),\n",
    "        new Tuple2([0.0, 2.0], [0.0, 1.0]),\n",
    "        new Tuple2([2.0],[2.0]),\n",
    "        new Tuple2([2.0, 0.0], [2.0, 0.0]),\n",
    "        new Tuple2([0.0, 1.0, 2.0], [0.0, 1.0]),\n",
    "        new Tuple2([1.0], [1.0, 2.0])\n",
    "    ];\n",
    "\n",
    "    var scoreAndLabels = sc.parallelize(data);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Instantiate metrics object\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var metrics = new MultilabelMetrics(scoreAndLabels);\n",
    "\n",
    "var result = metrics;\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Summary stats\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    print(\"Recall = \" + result.recall());\n",
    "    print(\"Precision = \", +result.precision());\n",
    "    print(\"F1 measure = \", +result.f1Measure());\n",
    "    print(\"Accuracy = \", +result.accuracy());\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Stats by labels\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    for (var i = 0; i < result.labels().length - 1; i++) {\n",
    "        print(\"Class \" + result.labels()[i] + \" precision = \" +  result.precision(result.labels()[i]));\n",
    "        print(\"Class \" + result.labels()[i] + \" recall = \" + result.recall(result.labels()[i]));\n",
    "        print(\"Class \" + result.labels()[i] + \" F1 score = \" +  result.f1Measure(result.labels()[i]));\n",
    "    }\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Micro stats\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    print(\"Micro recall = \" + result.microRecall());\n",
    "    print(\"Micro precision = \" + result.microPrecision());\n",
    "    print(\"Micro F1 measure = \" + result.microF1Measure());\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Hamming loss\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    print(\"Hamming loss = \" + result.hammingLoss());\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Subset accuracy\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    print(\"Subset accuracy = \" + result.subsetAccuracy());\n",
    "\n",
    "    sc.stop();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.6.1 (Javascript)",
   "language": "javascript",
   "name": "eclair"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}