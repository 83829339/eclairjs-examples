
{
 "cells": [
      
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    var SparkConf = require('eclairjs\/SparkConf');\n",
    "    var SparkContext = require('eclairjs\/SparkContext');\n",
    "    var sparkConf = new SparkConf().setAppName(\"JavaScript SimpleTextClassificationPipeline\");\n",
    "    var sc = new SparkContext(sparkConf);\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var SQLContext = require('eclairjs\/sql\/SQLContext');\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var Tokenizer = require('eclairjs\/ml\/feature\/Tokenizer');\n",
    "    var HashingTF = require('eclairjs\/ml\/feature\/HashingTF');\n",
    "    var LogisticRegression = require('eclairjs\/ml\/classification\/LogisticRegression');\n",
    "    var Pipeline = require('eclairjs\/ml\/Pipeline');\n",
    "    var PipelineModel = require('eclairjs\/ml\/PipelineModel');\n",
    "    var PipelineStage = require('eclairjs\/ml\/PipelineStage');\n",
    "    var StructType = require('eclairjs\/sql\/types\/StructType');\n",
    "    var StructField = require('eclairjs\/sql\/types\/StructField');\n",
    "    var DataTypes = require('eclairjs\/sql\/types').DataTypes;\n",
    "    var Metadata = require('eclairjs\/sql\/types\/Metadata');\n",
    "    var RowFactory = require('eclairjs\/sql\/RowFactory');\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var sqlContext = new SQLContext(sc);\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function LabeledDocument(id, text, label)\n",
    "   {\n",
    "    this.id=id;\n",
    "    this.text=text;\n",
    "    this.label=label;\n",
    "   }\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function Document(id, text)\n",
    "   {\n",
    "    this.id=id;\n",
    "    this.text=text;\n",
    "    }\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Prepare training documents, which are labeled.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var localTraining = [\n",
    "      new LabeledDocument(0 , \"a b c d e spark\", 1.0),\n",
    "      new LabeledDocument(1 , \"b d\", 0.0),\n",
    "      new LabeledDocument(2 , \"spark f g h\", 1.0),\n",
    "      new LabeledDocument(3 , \"hadoop mapreduce\", 0.0)];\n",
    "    var training = sqlContext.createDataFrameFromJson(sc.parallelize(localTraining), {\n",
    "        id:\"Integer\",\n",
    "        text:\"String\",\n",
    "        label:\"Double\"\n",
    "    });\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var tokenizer = new Tokenizer()\n",
    "      .setInputCol(\"text\")\n",
    "     .setOutputCol(\"words\");\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var hashingTF = new HashingTF()\n",
    "      .setNumFeatures(1000)\n",
    "      .setInputCol(tokenizer.getOutputCol())\n",
    "      .setOutputCol(\"features\");\n",
    "    var lr = new LogisticRegression()\n",
    "      .setMaxIter(10)\n",
    "      .setRegParam(0.001);\n",
    "    var pipeline = new Pipeline()\n",
    "      .setStages([ tokenizer, hashingTF, lr]);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Fit the pipeline to training documents.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var model = pipeline.fit(training);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Prepare test documents, which are unlabeled.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    localTest = [\n",
    "      new Document(4, \"spark i j k\"),\n",
    "      new Document(5, \"l m n\"),\n",
    "      new Document(6, \"spark hadoop spark\"),\n",
    "      new Document(7, \"apache hadoop\")];\n",
    "    var test = sqlContext.createDataFrameFromJson(sc.parallelize(localTest), {\n",
    "        id:\"Integer\",\n",
    "        text:\"String\"\n",
    "    });\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Make predictions on test documents.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var predictions = model.transform(test);\n",
    "    var rows = predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collect();\n",
    "var result = rows;\n",
    "\n",
    "    for (var i=0;i<result.length;i++)\n",
    "    {\n",
    "        var r=result[i];\n",
    "      print(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2) +\n",
    "           \", prediction=\" + r.get(3));\n",
    "\n",
    "    }\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $example off$\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    sc.stop();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.6.1 (Javascript)",
   "language": "javascript",
   "name": "eclair"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}