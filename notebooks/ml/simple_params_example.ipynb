
{
 "cells": [
      
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    var SparkConf = require('eclairjs\/SparkConf');\n",
    "    var SparkContext = require('eclairjs\/SparkContext');\n",
    "    var sparkConf = new SparkConf().setAppName(\"JavaScript SimpleParamsExample\");\n",
    "    var sc = new SparkContext(sparkConf);\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var SQLContext = require('eclairjs\/sql\/SQLContext');\n",
    "\n",
    "    var LabeledPoint = require('eclairjs\/mllib\/regression\/LabeledPoint');\n",
    "    var Vectors = require('eclairjs\/mllib\/linalg\/Vectors');\n",
    "    var LogisticRegression = require('eclairjs\/ml\/classification\/LogisticRegression');\n",
    "    var ParamMap = require('eclairjs\/ml\/param\/ParamMap');\n",
    "    var VectorUDT = require('eclairjs\/mllib\/linalg\/VectorUDT');\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var sqlContext = new SQLContext(sc);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Prepare training data.\n",
    " We use LabeledPoint, which is a JavaBean.  Spark SQL can convert RDDs of JavaBeans\n",
    " into DataFrames, where it uses the bean metadata to infer the schema.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var localTraining = [\n",
    "      new LabeledPoint(1.0, Vectors.dense(0.0, 1.1, 0.1)),\n",
    "      new LabeledPoint(0.0, Vectors.dense(2.0, 1.0, -1.0)),\n",
    "      new LabeledPoint(0.0, Vectors.dense(2.0, 1.3, 1.0)),\n",
    "      new LabeledPoint(1.0, Vectors.dense(0.0, 1.2, -0.5))];\n",
    "    var training = sqlContext.createDataFrameFromJson(sc.parallelize(localTraining), {\n",
    "      \"label\" : \"Double\",\n",
    "      \"features\" : new VectorUDT()\n",
    "    });\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a LogisticRegression instance.  This instance is an Estimator.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var lr = new LogisticRegression();\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Print out the parameters, documentation, and any default values.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\");\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We may set parameters using setter methods.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    lr.setMaxIter(10)\n",
    "      .setRegParam(0.01);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Learn a LogisticRegression model.  This uses the parameters stored in lr.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var model1 = lr.fit(training);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Since model1 is a Model (i.e., a Transformer produced by an Estimator),\n",
    " we can view the parameters it used during fit().\n",
    " This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
    " LogisticRegression instance.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    print(\"Model 1 was fit using parameters: \" + model1.parent().extractParamMap());\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We may alternatively specify parameters using a ParamMap.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var paramMap = new ParamMap();\n",
    "    paramMap.put(lr.maxIter().w(20)); \/\/ Specify 1 Param.\n",
    "    paramMap.put(lr.maxIter(), 30); \/\/ This overwrites the original maxIter.\n",
    "    var thresholds  = [0.45, 0.55];\n",
    "    paramMap.put(lr.regParam().w(0.1), lr.thresholds().w(thresholds)); \/\/ Specify multiple Params.\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " One can also combine ParamMaps.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var paramMap2 = new ParamMap();\n",
    "    paramMap2.put(lr.probabilityCol().w(\"myProbability\")); \/\/ Change output column name\n",
    "    var paramMapCombined = paramMap.$plus$plus(paramMap2);\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now learn a new model using the paramMapCombined parameters.\n",
    " paramMapCombined overrides all parameters set earlier via lr.set* methods.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var model2 = lr.fit(training, paramMapCombined);\n",
    "    print(\"Model 2 was fit using parameters: \" + model2.parent().extractParamMap());\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Prepare test documents.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var localTest = [\n",
    "        new LabeledPoint(1.0, Vectors.dense(-1.0, 1.5, 1.3)),\n",
    "        new LabeledPoint(0.0, Vectors.dense(3.0, 2.0, -0.1)),\n",
    "        new LabeledPoint(1.0, Vectors.dense(0.0, 2.2, -1.5))];\n",
    "    var test = sqlContext.createDataFrameFromJson(sc.parallelize(localTest), {\n",
    "      \"label\" : \"Double\",\n",
    "      \"features\" : new VectorUDT()\n",
    "    });\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Make predictions on test documents using the Transformer.transform() method.\n",
    " LogisticRegressionModel.transform will only use the 'features' column.\n",
    " Note that model2.transform() outputs a 'myProbability' column instead of the usual\n",
    " 'probability' column since we renamed the lr.probabilityCol parameter previously.\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    var results = model2.transform(test);\n",
    "\n",
    "    var rows=results.select(\"features\", \"label\", \"myProbability\", \"prediction\").collect();\n",
    "var result = rows;\n",
    "\n",
    "    for (var i=0;i<result.length;i++)\n",
    "    {\n",
    "      var r=result[i];\n",
    "      print(\"(\" + r.get(0) + \", \" + r.get(1) + \") -> prob=\" + r.get(2) +\n",
    "          \", prediction=\" + r.get(3));\n",
    "    }\n"
   ]

  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $example off$\n"
   ]

  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    sc.stop();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.6.1 (Javascript)",
   "language": "javascript",
   "name": "eclair"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}